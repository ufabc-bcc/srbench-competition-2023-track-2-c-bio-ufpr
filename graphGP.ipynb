{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Neural Network\n",
    "import torch_geometric as pyG\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# utils\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bike dataset from datasets folder\n",
    "bike_data = pd.read_csv('datasets/MontrealBikeLane.csv')\n",
    "# removing columns that have any missing data\n",
    "bike_data = bike_data.dropna(axis=1)\n",
    "bike_data[\"Date\"] = pd.to_datetime(bike_data[\"Date\"], format=\"%d/%m/%Y\")\n",
    "bike_data = bike_data.sort_values(by=['Date', 'Time'], ascending=True)\n",
    "bike_data = bike_data.reset_index(drop=True)\n",
    "bike_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_data.loc[:, [col not in  [\"Time\"] for col in bike_data.columns]].plot(x=\"Date\", figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with column names and index order starting from 0 excluding Date and Time\n",
    "# this is because we want to predict the number of bikes in the future\n",
    "# and we don't want to include the future in our features\n",
    "node_encoder = {}\n",
    "for i, col in enumerate(bike_data.columns):\n",
    "    if col != 'Date' and col != 'Time':\n",
    "        node_encoder[col] = i - 2\n",
    "node_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the BikeLanesConnections.txt and extract the connections between the nodes via links in the PyG format.\n",
    "# Use the previous function of node enconder to transforms bike lanes name to the desired encoded format.\n",
    "# Path: BikeLanesConnections.txt, format X <-> Y\n",
    "# X and Y are the names of the bike lanes\n",
    "# X <-> Y means that X and Y are connected\n",
    "def get_edge_connections():\n",
    "    with open('BikeLanesConnections.txt') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [x.split(' <-> ') for x in content]\n",
    "    content = [[node_encoder[x[0]], node_encoder[x[1]]] for x in content]\n",
    "    content = torch.tensor(content).t().contiguous()\n",
    "    return content\n",
    "\n",
    "# create function to get edge and node index\n",
    "def get_node_and_edge_index(bike_data):\n",
    "    node_index = list(\n",
    "        range(0, len(\n",
    "                    [col for col in bike_data.columns if col not in [\"Date\", \"Time\"]]\n",
    "                )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    edge_index = get_edge_connections()\n",
    "\n",
    "    return node_index, edge_index\n",
    "\n",
    "\n",
    "# create function to create a Data object in Pytorch Geometric for each row\n",
    "def get_all_nodes_features_and_labels(dataframe, k = 12):\n",
    "    \"\"\"\n",
    "    Each feature vector from the nodes is the vector of lagged values of the\n",
    "    bike count from previous k Date. It assumes the dataframe is ordered by Date and Time.\n",
    "    \"\"\"\n",
    "    all_nodes_features = []\n",
    "    all_nodes_labels = []\n",
    "    for j in [col for col in dataframe.columns if col not in [\"Date\", \"Time\"]]:\n",
    "        node_features_across_dates = []\n",
    "        node_labels_across_dates = []\n",
    "        for i in range(dataframe.shape[0]):\n",
    "            # get the previous k values\n",
    "            lagged_values = dataframe[j].iloc[i-k:i].values\n",
    "            lagged_values = torch.tensor(lagged_values).float()\n",
    "            label_value = torch.tensor(dataframe[j].iloc[i]).float()\n",
    "            # if there are not k previous values, skip the row\n",
    "            if len(lagged_values) == k:\n",
    "                node_features_across_dates.append(lagged_values)\n",
    "                node_labels_across_dates.append(label_value)\n",
    "        all_nodes_features.append(node_features_across_dates)\n",
    "        all_nodes_labels.append(node_labels_across_dates)\n",
    "\n",
    "    return all_nodes_features, all_nodes_labels\n",
    "\n",
    "# showcasing one graph\n",
    "node_index, edge_index = get_node_and_edge_index(bike_data)\n",
    "all_nodes_features, all_nodes_labels = get_all_nodes_features_and_labels(bike_data, k=7)\n",
    "print(all_nodes_features[0][0], all_nodes_labels[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []\n",
    "for date_index in range(len(all_nodes_features[0])):\n",
    "    node_features = [all_nodes_features[i][date_index] for i in range(len(all_nodes_features))]\n",
    "    node_labels = [all_nodes_labels[i][date_index] for i in range(len(all_nodes_labels))]\n",
    "    graph = Data(x=torch.stack(node_features), y=torch.stack(node_labels), edge_index=edge_index)\n",
    "    graph_list.append(graph)\n",
    "print(len(graph_list))\n",
    "graph = graph_list[0]\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_networkx(graph, n_sample=None):\n",
    "    g = to_networkx(graph, node_attrs=[\"x\"])\n",
    "    y = graph.x.numpy()\n",
    "\n",
    "    if n_sample is not None:\n",
    "        sampled_nodes = random.sample(g.nodes, n_sample)\n",
    "        g = g.subgraph(sampled_nodes)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def plot_graph(g):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    nx.draw_planar(g, node_size=30, arrows=False, node_color=None)\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "g = convert_to_networkx(graph, n_sample=None)\n",
    "plot_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNodeSplitter:\n",
    "    def __init__(self, train_val_test_ratio = [0.8, 0.1, 0.1]):\n",
    "        self.train_val_test_ratio = train_val_test_ratio\n",
    "        self.has_mask = False\n",
    "    \n",
    "\n",
    "    def setup_split(self, num_nodes) -> None:\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "        # randomly assign nodes to train, val, test with permutation\n",
    "        train_ratio = self.train_val_test_ratio[0]\n",
    "        train_n = int(num_nodes * train_ratio)\n",
    "        val_ratio = self.train_val_test_ratio[1]\n",
    "        val_n = int(num_nodes * val_ratio)\n",
    "\n",
    "        permutation = torch.randperm(num_nodes)\n",
    "        train_mask[permutation[:train_n]] = 1\n",
    "        val_mask[permutation[train_n:train_n+val_n]] = 1\n",
    "        test_mask[permutation[train_n+val_n:]] = 1\n",
    "\n",
    "        self.train_mask = train_mask\n",
    "        self.val_mask = val_mask\n",
    "        self.test_mask = test_mask\n",
    "\n",
    "        self.has_mask = True\n",
    "\n",
    "\n",
    "    def __call__(self, graph, save_mask=True) -> pyG.data.data.Data:\n",
    "        if self.has_mask is False or save_mask is False:\n",
    "            print(\"Generating new random split.\")\n",
    "            self.setup_split(graph.num_nodes)\n",
    "\n",
    "        # assign masks to graph\n",
    "        graph.train_mask = self.train_mask\n",
    "        graph.val_mask = self.val_mask\n",
    "        graph.test_mask = self.test_mask\n",
    "\n",
    "        return graph\n",
    "\n",
    "split = RandomNodeSplitter()\n",
    "graph = split(graph)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using loader to get the full dataset but split into train, val, test\n",
    "split = RandomNodeSplitter()\n",
    "split.setup_split(graph.num_nodes)\n",
    "graph_list = [split(graph) for graph in graph_list]\n",
    "loader = DataLoader(graph_list, batch_size=len(graph_list), shuffle=False)\n",
    "for dataset in loader:\n",
    "    break\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphGP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
